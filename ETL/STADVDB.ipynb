{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YELP DATASET DIMENSIONAL MODEL ETL PROCESSING\n",
    "\n",
    "## STADVDB S12\n",
    "- Cayton, Alenna\n",
    "- Gubat, Angeline\n",
    "- Pascual, Jeremy\n",
    "- Pua, Shaun   \n",
    "\n",
    "The students will perform a ETL processing, on the group's created star schema dimensional model based on the YELP dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "The dataset that was used was retrieved from **https://relational.fit.cvut.cz/**, an online repository of publicly available data sets. The students chose to use the dataset entitled **\"YelpDataset3\"** and chose **1 fact table** and **3 dimensional tables** to create their dimensional model making use of a **star schema**. The specifics of the table are as follows. The **fact table** is titled **Business** with 11 columns, full_address, active, city, review_count, business_name, neighborhoods, latitude, longitude, state, and stars. The 3 other dimension tables are titled **Business_hours** with the columns business_id, day_of_week, opening_time_hours, and closing_time_hours, **Business_Attributes** with the columns business_id, attribute_name, and attribute_value, and **Business_Categories** with the columns business_id and category. All tables have **business_id as their primary keys**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "### Cleaning the dataset\n",
    "We first clean the dataset by reformating it, dealing with missing data, getting rid of columns that we will most likely not use, and removing duplicate data if applicable in order to extract useful information by using proper data manipulation functions and techniques\n",
    "### Importing libraries and the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Please change the directory below to the location where you have stored this notebook\n",
    "Business_df = pd.read_csv('C:/Users/MyComputer/Downloads/STADVDB MCO2/ETL/yelp-business.csv')\n",
    "Business_attributes_df = pd.read_csv('C:/Users/MyComputer/Downloads/STADVDB MCO2/ETL/yelp-business-attributes.csv')\n",
    "Business_categories_df = pd.read_csv('C:/Users/MyComputer/Downloads/STADVDB MCO2/ETL/yelp-business-categories.csv')\n",
    "Business_hours_df = pd.read_csv('C:/Users/MyComputer/Downloads/STADVDB MCO2/ETL/yelp-business-hours.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the libraries and the dataset, We will now remove data or columns that serve no real purpose or have null values that were checked beforehand. They are the following: \n",
    "\n",
    "table:**Business**\n",
    " column:*neighborhoods*. \n",
    "\n",
    "Following this we will check to see if the values for certain columns are in the acceptable value range.\n",
    "### Dropping useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Business_df.drop(Business_df.columns[[6]], axis=1, inplace=True)\n",
    "#Please remove the \"#\" symbol from the line below to see what columns are left in the dataset\n",
    "#Business_df.info()\n",
    "#Business_hours_df.info()\n",
    "#Business_attributes_df.info()\n",
    "#Business_categories_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the range of values for variables with specific value ranges and checking if any dataframes have null or NaN values and duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Business_df['active'].unique()\n",
    "#Business_df['stars'].unique()\n",
    "#Business_df['review_count'].unique()\n",
    "#Business_attributes_df['category'].unique()\n",
    "#Business_categories_df['day_of_week'].unique()\n",
    "#Business_categories_df['opening_time_hours'].unique()\n",
    "#Business_categories_df['closing_time_hours'].unique()\n",
    "\n",
    "#Business_df.isnull().values.any()\n",
    "#Business_hours_df.isnull().values.any()\n",
    "#Business_attributes_df.isnull().values.any()\n",
    "#Business_categories_df.isnull().values.any()\n",
    "\n",
    "#Business_df.duplicated(subset=None, keep='first')\n",
    "#Business_attributes_df.duplicated(subset=None, keep='first')\n",
    "#Business_hours_df.duplicated(subset=None, keep='first')\n",
    "#Business_categories_df.duplicated(subset=None, keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have discovered that all of the variables listed above have no missing values and have acceptable values in respect to their columns and no duplicates are found\n",
    "\n",
    "***If you wish to see how we did this please remove the \"#\" symbol in the lines above and then press shift followed by enter***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "We will now export our cleaned tables for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'Business_attributes_YELP_clean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6d9487ca9716>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mBusiness_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'Business_YELP_clean.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mBusiness_hours_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Business_hours_YELP_clean.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mBusiness_attributes_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Business_attributes_YELP_clean.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mBusiness_categories_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Business_categories_YELP_clean.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Business_df.to_csv(r'Business_YELP_clean.csv',index=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m         )\n\u001b[1;32m-> 3204\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             )\n\u001b[0;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'Business_attributes_YELP_clean.csv'"
     ]
    }
   ],
   "source": [
    "Business_df.to_csv(r'Business_YELP_clean.csv')\n",
    "Business_hours_df.to_csv('Business_hours_YELP_clean.csv')\n",
    "Business_attributes_df.to_csv('Business_attributes_YELP_clean.csv')\n",
    "Business_categories_df.to_csv('Business_categories_YELP_clean.csv')\n",
    "#Business_df.to_csv(r'Business_YELP_clean.csv',index=False)\n",
    "#Business_hours_df.to_csv('Business_hours_YELP_clean.csv',index=False)\n",
    "#Business_attributes_df.to_csv('Business_attributes_YELP_clean.csv',index=False)\n",
    "#Business_categories_df.to_csv('Business_categories_YELP_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
